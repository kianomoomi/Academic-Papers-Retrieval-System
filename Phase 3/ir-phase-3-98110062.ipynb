{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر \n",
    "    crawling \n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و \n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک \n",
    "    task \n",
    "     در مورد \n",
    "    recommendation system \n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler \n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (22.10.0)\n",
      "Requirement already satisfied: cryptography>=3.4.6 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (41.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (23.2.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (23.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (2.1.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (6.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (67.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (21.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (3.4.4)\n",
      "Requirement already satisfied: lxml>=4.3.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (4.9.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cryptography>=3.4.6->scrapy) (1.15.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.6.3)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.3)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->scrapy) (2.4.7)\n",
      "Requirement already satisfied: idna in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (2.28.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (3.9.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.20)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)\n",
      "     ---------------------------------------- 6.7/6.7 MB 63.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from selenium) (1.26.13)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "     ------------------------------------ 384.9/384.9 kB 216.1 kB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting async-generator>=1.9 (from trio~=0.17->selenium)\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium)\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\kian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Installing collected packages: sortedcontainers, wsproto, PySocks, outcome, exceptiongroup, async-generator, trio, trio-websocket, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 exceptiongroup-1.1.1 outcome-1.2.0 selenium-4.10.0 sortedcontainers-2.4.0 trio-0.22.0 trio-websocket-0.10.3 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromedriver-py\n",
      "  Downloading chromedriver_py-114.0.5735.90-py3-none-any.whl (30.3 MB)\n",
      "     -------------------------------------- 30.3/30.3 MB 697.7 kB/s eta 0:00:00\n",
      "Installing collected packages: chromedriver-py\n",
      "Successfully installed chromedriver-py-114.0.5735.90\n"
     ]
    }
   ],
   "source": [
    "!pip install chromedriver-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from chromedriver_py import binary_path\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soleymani\n",
    "soleymani_queue = []\n",
    "with open (\"Soleymani.txt\", \"r\") as myfile:\n",
    "    for i in range(5):\n",
    "        line = myfile.readline()\n",
    "        soleymani_queue.append(line)\n",
    "    myfile.close()\n",
    "\n",
    "# sharifi\n",
    "sharifi_queue = []\n",
    "with open (\"Sharifi.txt\", \"r\") as myfile:\n",
    "    for i in range(5):\n",
    "        line = myfile.readline()\n",
    "        sharifi_queue.append(line)\n",
    "    myfile.close()\n",
    "\n",
    "# rohban\n",
    "rohban_queue = []\n",
    "with open (\"Rohban.txt\", \"r\") as myfile:\n",
    "    for i in range(5):\n",
    "        line = myfile.readline()\n",
    "        rohban_queue.append(line)\n",
    "    myfile.close()\n",
    "\n",
    "# rabiee\n",
    "rabiee_queue = []\n",
    "with open (\"Rabiee.txt\", \"r\") as myfile:\n",
    "    for i in range(5):\n",
    "        line = myfile.readline()\n",
    "        rabiee_queue.append(line)\n",
    "    myfile.close()\n",
    "\n",
    "# kasaei\n",
    "kasaei_queue = []\n",
    "with open (\"Kasaei.txt\", \"r\") as myfile:\n",
    "    for i in range(5):\n",
    "        line = myfile.readline()\n",
    "        kasaei_queue.append(line)\n",
    "    myfile.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_object = Service(binary_path)\n",
    "driver = webdriver.Chrome(service=service_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "i = 0\n",
    "soleymani_papers_info = []\n",
    "sharifi_papers_info = []\n",
    "rohban_papers_info = []\n",
    "rabiee_papers_info = []\n",
    "kasaei_papers_info = []\n",
    "while counter < 2000:\n",
    "    if i == 0:\n",
    "        current_url = soleymani_queue.pop(0)\n",
    "    elif i == 1:\n",
    "        current_url = sharifi_queue.pop(0)\n",
    "    elif i == 2:\n",
    "        current_url = rohban_queue.pop(0)\n",
    "    elif i == 3:\n",
    "        current_url = rabiee_queue.pop(0)\n",
    "    elif i == 4:\n",
    "        current_url = kasaei_queue.pop(0)\n",
    "    driver.get(current_url)\n",
    "    time.sleep(2)\n",
    "    # driver.implicitly_wait(5)\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    try:\n",
    "        title = None\n",
    "        abstract = None\n",
    "        publication_year = None\n",
    "        authors = []\n",
    "        data = soup.find_all('meta')\n",
    "        for tag in data:\n",
    "            if tag.get('name', None) == 'citation_title':\n",
    "                title = tag.get('content', None)\n",
    "            elif tag.get('name', None) == 'description':\n",
    "                abstract = tag.get('content', None)\n",
    "            elif tag.get('name', None) == 'citation_publication_date':\n",
    "                publication_year = tag.get('content', None)\n",
    "            elif tag.get('name', None) == 'citation_author':\n",
    "                authors.append(tag.get('content', None))\n",
    "        counts = soup.find_all(class_ = \"dropdown-filters__result-count__header dropdown-filters__result-count__citations\")\n",
    "        citation_count = counts[0].string.split(' ')[0]\n",
    "        refrences_count = counts[1].string.split(' ')[0]   \n",
    "        reference_papers_tag = soup.find(id = 'cited-papers')\n",
    "        reference_papers_tag_containing_url = reference_papers_tag.find_all(class_ = 'cl-paper-row citation-list__paper-row paper-v2-font-only')\n",
    "        references = reference_papers_tag.find_all(class_ = 'cl-paper-title')\n",
    "        references_urls = []\n",
    "        references_ids = []\n",
    "        reference_count = 0\n",
    "        for tag in reference_papers_tag_containing_url:\n",
    "            reference_count += 1\n",
    "            if reference_count > 10:\n",
    "                break\n",
    "            references_urls.append('https://www.semanticscholar.org' + tag.find('a')['href'])\n",
    "            references_ids.append(tag.find('a')['href'].split('/')[3])\n",
    "\n",
    "\n",
    "        related_papers_tag= soup.find_all('li', class_ = 'paper-meta-item')\n",
    "        related_topics = []\n",
    "        for tag in related_papers_tag:\n",
    "            if tag.find('span') == None:\n",
    "                related_topics_as_string = tag.text\n",
    "                break\n",
    "        if related_topics_as_string.split(',') == None:\n",
    "            related_topics.append(related_topics_as_string)\n",
    "        else:\n",
    "            related_topics = related_topics_as_string.split(',')[:5]\n",
    "            related_topics = [topic.strip() for topic in related_topics]\n",
    "\n",
    "        id = current_url.split('/')[5]\n",
    "\n",
    "        paper_info = { 'ID': id, 'Title': title, 'Abstract': abstract, 'Publication Year': publication_year, 'Authors': authors, \n",
    "                        'Related Topics': related_topics, 'Citation Count': citation_count, 'Refrences Count': refrences_count,\n",
    "                        'References': references_ids}\n",
    "        if i == 0:\n",
    "            soleymani_papers_info.append(paper_info)\n",
    "            soleymani_queue.extend(references_urls)\n",
    "        elif i == 1:\n",
    "            sharifi_papers_info.append(paper_info)\n",
    "            sharifi_queue.extend(references_urls)\n",
    "        elif i == 2:\n",
    "            rohban_papers_info.append(paper_info)\n",
    "            rohban_queue.extend(references_urls)\n",
    "        elif i == 3:\n",
    "            rabiee_papers_info.append(paper_info)\n",
    "            rabiee_queue.extend(references_urls)\n",
    "        elif i == 4:\n",
    "            kasaei_papers_info.append(paper_info)\n",
    "            kasaei_queue.extend(references_urls)\n",
    "    except Exception as e:\n",
    "        i = (i + 1) % 5\n",
    "        counter += 1\n",
    "        continue\n",
    "    i = (i + 1) % 5\n",
    "    counter += 1\n",
    "\n",
    "driver.quit()\n",
    "with open('crawled_paper_soleymani.json', 'w') as outfile:\n",
    "    json.dump(soleymani_papers_info, outfile)\n",
    "with open('crawled_paper_sharifi.json', 'w') as outfile:\n",
    "    json.dump(sharifi_papers_info, outfile)\n",
    "with open('crawled_paper_rohban.json', 'w') as outfile:\n",
    "    json.dump(rohban_papers_info, outfile)\n",
    "with open('crawled_paper_rabiee.json', 'w') as outfile:\n",
    "    json.dump(rabiee_papers_info, outfile)\n",
    "with open('crawled_paper_kasaei.json', 'w') as outfile:\n",
    "    json.dump(kasaei_papers_info, outfile)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank \n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم \n",
    "    PageRank \n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم \n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pagerank(graph: Dict[str, List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
    "\n",
    "    Parameters:\n",
    "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
    "    \n",
    "    Returns:\n",
    "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
    "    \"\"\"\n",
    "\n",
    "    dictionary_id_to_int_id = {}\n",
    "    int_id = 0\n",
    "    for node in graph.keys():\n",
    "        if node not in dictionary_id_to_int_id:\n",
    "            dictionary_id_to_int_id[node] = int_id\n",
    "            int_id += 1\n",
    "        for reference in graph[node]:\n",
    "            if reference not in dictionary_id_to_int_id:\n",
    "                dictionary_id_to_int_id[reference] = int_id\n",
    "                int_id += 1\n",
    "\n",
    "    P_matrix = np.zeros((len(dictionary_id_to_int_id), len(dictionary_id_to_int_id)))\n",
    "    for node in graph.keys():\n",
    "        references = graph[node]\n",
    "        index = dictionary_id_to_int_id[node]\n",
    "        if len(references) == 0:\n",
    "            P_matrix[index, :] = 1 / len(dictionary_id_to_int_id)\n",
    "        else:\n",
    "            for reference in references:\n",
    "                P_matrix[index, dictionary_id_to_int_id[reference]] = 1 / len(references)\n",
    "    \n",
    "    v_matrix =np.ones((len(dictionary_id_to_int_id), len(dictionary_id_to_int_id))) / len(dictionary_id_to_int_id)\n",
    "    alpha = 0.85\n",
    "    P_matrix = alpha * P_matrix + (1 - alpha) * v_matrix\n",
    "    x_0 = np.zeros((len(dictionary_id_to_int_id), 1))\n",
    "    x_0[0] = 1\n",
    "    x = x_0\n",
    "    for i in range(100):\n",
    "        x = np.matmul(P_matrix, x)\n",
    "    dictionary_int_id_to_id = {value: key for key, value in dictionary_id_to_int_id.items()}\n",
    "    dictionary_id_to_pagerank = {}\n",
    "    for i in range(len(x)):\n",
    "        dictionary_id_to_pagerank[dictionary_int_id_to_id[i]] = x[i][0]\n",
    "    return dictionary_id_to_pagerank\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم \n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد \n",
    "خاص استفاده می‌کنیم. این تابع، یک \n",
    "    field \n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paper_by_id(id, papers):\n",
    "    for paper in papers:\n",
    "        if paper['ID'] == id:\n",
    "            return paper\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['91d43e2f2097574039fa52a75d1105d4f885d68a',\n",
       " '4c8e87d996826725192d15273c14bc0f16a6f17e',\n",
       " 'd8b8805b824a17471f6005c838806ba3c892dd08',\n",
       " '547c854985629cfa9404a5ba8ca29367b5f8c25f',\n",
       " '95df3ed3f0be8092166385467b4c60bce1b7673f',\n",
       " 'c55585871acc109ce1dfe4d66a61d7d6baa7f58e',\n",
       " 'f4cbe92def586e12b22f8e31109d1dd6ffa5e86d',\n",
       " '3e36828fb7c7d9239afb13273b2988381abeece6',\n",
       " '9c24454b071bc8e96ea46c5064a7bddf07cca464',\n",
       " 'c6bb1291dc4d69d8362d05737dcd97020832c8d4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def important_articles(Professor: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
    "\n",
    "    Parameters:\n",
    "    Professor (str): Professor's name.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
    "    \"\"\"\n",
    "\n",
    "    if Professor == 'Soleymani':\n",
    "        with open('crawled_paper_soleymani.json') as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    elif Professor == 'Sharifi':\n",
    "        with open('crawled_paper_sharifi.json') as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    elif Professor == 'Rohban':\n",
    "        with open('crawled_paper_rohban.json') as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    elif Professor == 'Rabiee':\n",
    "        with open('crawled_paper_rabiee.json') as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    elif Professor == 'Kasaei':\n",
    "        with open('crawled_paper_kasaei.json') as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    graph = {}\n",
    "    while(len(papers_info) > 0):\n",
    "        paper = papers_info.pop(0)\n",
    "        graph[paper['ID']] = paper['References']\n",
    "        for reference_id in paper['References']:\n",
    "            reference = find_paper_by_id(reference_id, papers_info)\n",
    "            if reference is None:\n",
    "                graph[reference_id] = []\n",
    "            elif reference_id in graph:\n",
    "                continue\n",
    "            else:\n",
    "                papers_info.append(reference)\n",
    "    dictionary_id_to_pagerank = pagerank(graph)\n",
    "    sorted_pagerank = sorted(dictionary_id_to_pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [paper[0] for paper in sorted_pagerank[:10]]\n",
    "        \n",
    "\n",
    "# top 10 important articles in the field of professor\n",
    "important_articles('Rohban')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled_paper_soleymani.json') as json_file:\n",
    "    soleymani_papers_info = json.load(json_file)\n",
    "soleymani_graph = {}\n",
    "while(len(soleymani_papers_info) > 0):\n",
    "    paper = soleymani_papers_info.pop(0)\n",
    "    soleymani_graph[paper['ID']] = paper['References']\n",
    "    for reference_id in paper['References']:\n",
    "        reference = find_paper_by_id(reference_id, soleymani_papers_info)\n",
    "        if reference is None:\n",
    "            soleymani_graph[reference_id] = []\n",
    "        elif reference_id in soleymani_graph:\n",
    "            continue\n",
    "        else:\n",
    "            soleymani_papers_info.append(reference)\n",
    "soleymani_pagerank = pagerank(soleymani_graph)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled_paper_sharifi.json') as json_file:\n",
    "    sharifi_papers_info = json.load(json_file)\n",
    "sharifi_graph = {}\n",
    "while(len(sharifi_papers_info) > 0):\n",
    "    paper = sharifi_papers_info.pop(0)\n",
    "    sharifi_graph[paper['ID']] = paper['References']\n",
    "    for reference_id in paper['References']:\n",
    "        reference = find_paper_by_id(reference_id, sharifi_papers_info)\n",
    "        if reference is None:\n",
    "            sharifi_graph[reference_id] = []\n",
    "        elif reference_id in sharifi_graph:\n",
    "            continue\n",
    "        else:\n",
    "            sharifi_papers_info.append(reference)\n",
    "sharifi_pagerank = pagerank(sharifi_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled_paper_rohban.json') as json_file:\n",
    "    rohban_papers_info = json.load(json_file)\n",
    "rohban_graph = {}\n",
    "while(len(rohban_papers_info) > 0):\n",
    "    paper = rohban_papers_info.pop(0)\n",
    "    rohban_graph[paper['ID']] = paper['References']\n",
    "    for reference_id in paper['References']:\n",
    "        reference = find_paper_by_id(reference_id, rohban_papers_info)\n",
    "        if reference is None:\n",
    "            rohban_graph[reference_id] = []\n",
    "        elif reference_id in rohban_graph:\n",
    "            continue\n",
    "        else:\n",
    "            rohban_papers_info.append(reference)\n",
    "rohban_pagerank = pagerank(rohban_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled_paper_rabiee.json') as json_file:\n",
    "    rabiee_papers_info = json.load(json_file)\n",
    "rabiee_graph = {}\n",
    "while(len(rabiee_papers_info) > 0):\n",
    "    paper = rabiee_papers_info.pop(0)\n",
    "    rabiee_graph[paper['ID']] = paper['References']\n",
    "    for reference_id in paper['References']:\n",
    "        reference = find_paper_by_id(reference_id, rabiee_papers_info)\n",
    "        if reference is None:\n",
    "            rabiee_graph[reference_id] = []\n",
    "        elif reference_id in rabiee_graph:\n",
    "            continue\n",
    "        else:\n",
    "            rabiee_papers_info.append(reference)\n",
    "rabiee_pagerank = pagerank(rabiee_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled_paper_kasaei.json') as json_file:\n",
    "    kasaei_papers_info = json.load(json_file)\n",
    "kasaei_graph = {}\n",
    "while(len(kasaei_papers_info) > 0):\n",
    "    paper = kasaei_papers_info.pop(0)\n",
    "    kasaei_graph[paper['ID']] = paper['References']\n",
    "    for reference_id in paper['References']:\n",
    "        reference = find_paper_by_id(reference_id, kasaei_papers_info)\n",
    "        if reference is None:\n",
    "            kasaei_graph[reference_id] = []\n",
    "        elif reference_id in kasaei_graph:\n",
    "            continue\n",
    "        else:\n",
    "            kasaei_papers_info.append(reference)\n",
    "kasaei_pagerank = pagerank(kasaei_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled_paper_soleymani.json') as json_file:\n",
    "    soleymani_papers_info = json.load(json_file)\n",
    "with open('crawled_paper_sharifi.json') as json_file:\n",
    "    sharifi_papers_info = json.load(json_file)\n",
    "with open('crawled_paper_rohban.json') as json_file:\n",
    "    rohban_papers_info = json.load(json_file)\n",
    "with open('crawled_paper_rabiee.json') as json_file:\n",
    "    rabiee_papers_info = json.load(json_file)\n",
    "with open('crawled_paper_kasaei.json') as json_file:\n",
    "    kasaei_papers_info = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "for paper in soleymani_papers_info:\n",
    "    all_titles.append(paper['Title'])\n",
    "for paper in sharifi_papers_info:\n",
    "    all_titles.append(paper['Title'])\n",
    "for paper in rohban_papers_info:\n",
    "    all_titles.append(paper['Title'])\n",
    "for paper in rabiee_papers_info:\n",
    "    all_titles.append(paper['Title'])\n",
    "for paper in kasaei_papers_info:\n",
    "    all_titles.append(paper['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_abstracts = []\n",
    "for paper in soleymani_papers_info:\n",
    "    all_abstracts.append(paper['Abstract'])\n",
    "for paper in sharifi_papers_info:\n",
    "    all_abstracts.append(paper['Abstract'])\n",
    "for paper in rohban_papers_info:\n",
    "    all_abstracts.append(paper['Abstract'])\n",
    "for paper in rabiee_papers_info:\n",
    "    all_abstracts.append(paper['Abstract'])\n",
    "for paper in kasaei_papers_info:\n",
    "    all_abstracts.append(paper['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.positions = []\n",
    "        \n",
    "class PositionalIndex:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def add(self, token, doc_id, position, is_title):\n",
    "        node = self.root\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.positions.append((doc_id, position, is_title))\n",
    "\n",
    "    def remove(self, doc_id):\n",
    "        def remove_helper(node, doc_id):\n",
    "            if not node:\n",
    "                return\n",
    "            for child in node.children.values():\n",
    "                remove_helper(child, doc_id)\n",
    "            node.positions = [pos for pos in node.positions if pos[0] != doc_id]\n",
    "        remove_helper(self.root, doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_positional_indexes(all_titles, all_abstracts):\n",
    "    \"\"\"\n",
    "    Get processed data and insert words in that into a trie and construct postional_index and posting lists after wards.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str\n",
    "        processed data \n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    docs: \n",
    "        list of docs with specificied id, title, abstract.\n",
    "    \"\"\"\n",
    "    index = PositionalIndex()\n",
    "    for i in range(len(all_titles)):\n",
    "        title = all_titles[i]\n",
    "        abstract = all_abstracts[i]\n",
    "        for j in range(len(title)):\n",
    "            index.add(title[j], i, j, True)\n",
    "        for j in range(len(abstract)):\n",
    "            index.add(abstract[j], i, j, False)\n",
    "    return index\n",
    "\n",
    "\n",
    "index = construct_positional_indexes(all_titles, all_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 points\n",
    "\n",
    "def get_posting_list(word: str):\n",
    "    \"\"\" get posting_list of a word\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word: str\n",
    "             word we want to check\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        dict \n",
    "            posting list\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Find posting list of a word\n",
    "    node = index.root\n",
    "    for char in word:\n",
    "        if char not in node.children:\n",
    "            break\n",
    "        node = node.children[char]\n",
    "    word_positions = node.positions\n",
    "    posting_list = {}\n",
    "    for doc_id, position, is_title in word_positions:\n",
    "        if doc_id not in posting_list:\n",
    "            posting_list[doc_id] = {'title': [], 'abstract': []}\n",
    "        if is_title:\n",
    "            posting_list[doc_id]['title'].append(position)\n",
    "        else:\n",
    "            posting_list[doc_id]['abstract'].append(position)\n",
    "    return posting_list\n",
    "\n",
    "len(get_posting_list('biology'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltn_lnn(title_query, abstract_query, max_result_count, weight, print_snippet, user_pagerank):\n",
    "    title_scores = {}\n",
    "    abstract_scores = {}\n",
    "    title_query_tokens = title_query.split()\n",
    "    for token in title_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['title'] != []:\n",
    "                if doc_id not in title_scores:\n",
    "                    title_scores[doc_id] = 0\n",
    "                tf = 1 + math.log(len(positions['title']), 10)\n",
    "                idf = math.log(len(all_titles) / len(posting_list), 10)\n",
    "                document_score = tf * idf\n",
    "                query_score = 1 + math.log(title_query_tokens.count(token), 10)\n",
    "                title_scores[doc_id] += document_score * query_score\n",
    "\n",
    "    abstract_query_tokens = abstract_query.split()\n",
    "    for token in abstract_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['abstract'] != []:\n",
    "                if doc_id not in abstract_scores:\n",
    "                    abstract_scores[doc_id] = 0\n",
    "                tf = 1 + math.log(len(positions['abstract']), 10)\n",
    "                idf = math.log(len(all_titles) / len(posting_list), 10)\n",
    "                document_score = tf * idf\n",
    "                query_score = 1 + \\\n",
    "                    math.log(abstract_query_tokens.count(token), 10)\n",
    "                abstract_scores[doc_id] += document_score * query_score\n",
    "    scores = {}\n",
    "    for doc_id in set(title_scores.keys()).union(set(abstract_scores.keys())):\n",
    "        title_score = title_scores.get(doc_id, 0)\n",
    "        abstract_score = abstract_scores.get(doc_id, 0)\n",
    "        # considering pagerank in scoring\n",
    "        final_score = weight * abstract_score + (1 - weight) * title_score + 100 * user_pagerank.get(doc_id, 0)\n",
    "        scores[doc_id] = final_score\n",
    "    scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    if max_result_count == -1:\n",
    "        max_result_count = len(scores)\n",
    "    top_results = scores[:max_result_count]\n",
    "    if print_snippet:\n",
    "        for doc_id, score in top_results:\n",
    "            print(\"id: \", doc_id)\n",
    "            title = \"\"\n",
    "            abstract = \"\"\n",
    "            if find_paper_by_id(doc_id, soleymani_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, soleymani_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, soleymani_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, sharifi_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, sharifi_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, sharifi_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, rohban_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, rohban_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, rohban_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, rabiee_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, rabiee_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, rabiee_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, kasaei_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, kasaei_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, kasaei_papers_info)['Abstract']\n",
    "            print(\"Title: \", title[:50])\n",
    "            print(\"Abstract: \", abstract[:50])\n",
    "            print(\"*******************\")\n",
    "            \n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltc_lnc(title_query, abstract_query, max_result_count, weight, print_snippet, user_pagerank):\n",
    "    title_scores = {}\n",
    "    abstract_scores = {}\n",
    "    title_query_tokens = title_query.split()\n",
    "    document_weights_title = []\n",
    "    query_weights_title = []\n",
    "    for token in title_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['title'] != []:\n",
    "                if doc_id not in title_scores:\n",
    "                    title_scores[doc_id] = 0\n",
    "                tf = 1 + math.log(len(positions['title']), 10)\n",
    "                idf = math.log(len(all_titles) / len(posting_list), 10)\n",
    "                document_score = tf * idf\n",
    "                document_weights_title.append(document_score)\n",
    "                query_score = 1 + math.log(title_query_tokens.count(token), 10)\n",
    "                query_weights_title.append(query_score)\n",
    "\n",
    "    document_weights_title = np.array(document_weights_title)\n",
    "    query_weights_title = np.array(query_weights_title)\n",
    "    document_weights_title = document_weights_title / np.linalg.norm(document_weights_title)\n",
    "    query_weights_title = query_weights_title / np.linalg.norm(query_weights_title)\n",
    "    for token in title_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['title'] != []:\n",
    "                if doc_id not in title_scores:\n",
    "                    title_scores[doc_id] = 0\n",
    "                title_scores[doc_id] += document_weights_title[title_query_tokens.index(token)] * query_weights_title[title_query_tokens.index(token)]\n",
    "\n",
    "    abstract_query_tokens = abstract_query.split()\n",
    "    document_weights_abstract = []\n",
    "    query_weights_abstract = []\n",
    "    for token in abstract_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['abstract'] != []:\n",
    "                if doc_id not in abstract_scores:\n",
    "                    abstract_scores[doc_id] = 0\n",
    "                tf = 1 + math.log(len(positions['abstract']), 10)\n",
    "                idf = math.log(len(all_titles) / len(posting_list), 10)\n",
    "                document_score = tf * idf\n",
    "                document_weights_abstract.append(document_score)\n",
    "                query_score = 1 + math.log(abstract_query_tokens.count(token), 10)\n",
    "                query_weights_abstract.append(query_score)\n",
    "\n",
    "    \n",
    "    document_weights_abstract = np.array(document_weights_abstract)\n",
    "    query_weights_abstract = np.array(query_weights_abstract)\n",
    "    document_weights_abstract = document_weights_abstract / np.linalg.norm(document_weights_abstract)\n",
    "    query_weights_abstract = query_weights_abstract / np.linalg.norm(query_weights_abstract)\n",
    "    for token in abstract_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['abstract'] != []:\n",
    "                if doc_id not in abstract_scores:\n",
    "                    abstract_scores[doc_id] = 0\n",
    "                abstract_scores[doc_id] += document_weights_abstract[abstract_query_tokens.index(token)] * query_weights_abstract[abstract_query_tokens.index(token)]\n",
    "    scores = {}\n",
    "    for doc_id in set(title_scores.keys()).union(set(abstract_scores.keys())):\n",
    "        title_score = title_scores.get(doc_id, 0)\n",
    "        abstract_score = abstract_scores.get(doc_id, 0)\n",
    "        # considering pagerank in scoring\n",
    "        final_score = weight * abstract_score + (1 - weight) * title_score + 100 * user_pagerank.get(doc_id, 0)\n",
    "        scores[doc_id] = final_score\n",
    "    scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    if max_result_count == -1:\n",
    "        max_result_count = len(scores)\n",
    "    top_results = scores[:max_result_count]\n",
    "    if print_snippet:\n",
    "        for doc_id, score in top_results:\n",
    "            print(\"id: \", doc_id)\n",
    "            title = \"\"\n",
    "            abstract = \"\"\n",
    "            if find_paper_by_id(doc_id, soleymani_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, soleymani_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, soleymani_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, sharifi_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, sharifi_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, sharifi_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, rohban_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, rohban_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, rohban_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, rabiee_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, rabiee_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, rabiee_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, kasaei_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, kasaei_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, kasaei_papers_info)['Abstract']\n",
    "            print(\"Title: \", title[:50])\n",
    "            print(\"Abstract: \", abstract[:50])\n",
    "            print(\"*******************\")\n",
    "\n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = {}\n",
    "with open('crawled_paper_soleymani.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_sharifi.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_rohban.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_rabiee.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_kasaei.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_docs_average_length():\n",
    "    docs_title_average_length = 0\n",
    "    docs_abstract_average_length = 0\n",
    "    for title in all_titles:\n",
    "        docs_title_average_length += len(title)\n",
    "    docs_title_average_length /= len(all_titles)\n",
    "    for abstract in all_abstracts:\n",
    "        docs_abstract_average_length += len(abstract)\n",
    "    docs_abstract_average_length /= len(all_abstracts)\n",
    "    return docs_title_average_length, docs_abstract_average_length\n",
    "\n",
    "    \n",
    "\n",
    "def okapi_bm25(title_query, abstract_query, max_result_count, weight, print_snippet, k1=1.2, b=0.75, user_pagerank={}):\n",
    "    title_scores = {}\n",
    "    abstract_scores = {}\n",
    "    docs_title_average_length, docs_abstract_average_length = calculate_docs_average_length()\n",
    "    title_query_tokens = title_query.split()\n",
    "    for token in title_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['title'] != []:\n",
    "                if doc_id not in title_scores:\n",
    "                    title_scores[doc_id] = 0\n",
    "                tf = len(positions['title'])\n",
    "                idf = math.log(len(all_titles) / len(posting_list), 10)\n",
    "                for doc in all_papers:\n",
    "                    if doc['ID'] == doc_id:\n",
    "                        current_doc = doc\n",
    "                        break\n",
    "                document_score = idf * tf * (k1 + 1) / (tf + k1 * (1 - b + b * len(current_doc['Title']) / docs_title_average_length))\n",
    "                title_scores[doc_id] += document_score\n",
    "\n",
    "    abstract_query_tokens = abstract_query.split()\n",
    "    for token in abstract_query_tokens:\n",
    "        posting_list = get_posting_list(token)\n",
    "        for doc_id, positions in posting_list.items():\n",
    "            if positions['abstract'] != []:\n",
    "                if doc_id not in abstract_scores:\n",
    "                    abstract_scores[doc_id] = 0\n",
    "                tf = len(positions['abstract'])\n",
    "                idf = math.log(len(all_titles) / len(posting_list), 10)\n",
    "                for doc in all_papers:\n",
    "                    if doc['ID'] == doc_id:\n",
    "                        current_doc = doc\n",
    "                        break\n",
    "                document_score = idf * tf * (k1 + 1) / (tf + k1 * (1 - b + b * len(current_doc['Abstract']) / docs_abstract_average_length))\n",
    "                abstract_scores[doc_id] += document_score\n",
    "    scores = {}\n",
    "    for doc_id in set(title_scores.keys()).union(set(abstract_scores.keys())):\n",
    "        title_score = title_scores.get(doc_id, 0)\n",
    "        abstract_score = abstract_scores.get(doc_id, 0)\n",
    "        # considering pagerank in scoring\n",
    "        final_score = weight * abstract_score + (1 - weight) * title_score + 100 * user_pagerank.get(doc_id, 0)\n",
    "        scores[doc_id] = final_score\n",
    "    scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    if max_result_count == -1:\n",
    "        max_result_count = len(scores)\n",
    "    top_results = scores[:max_result_count]\n",
    "    if print_snippet:\n",
    "        for doc in top_results:\n",
    "            print(\"id: \", doc[0])\n",
    "            title = \"\"\n",
    "            abstract = \"\"\n",
    "            if find_paper_by_id(doc_id, soleymani_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, soleymani_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, soleymani_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, sharifi_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, sharifi_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, sharifi_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, rohban_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, rohban_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, rohban_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, rabiee_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, rabiee_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, rabiee_papers_info)['Abstract']\n",
    "            elif find_paper_by_id(doc_id, kasaei_papers_info) is not None:\n",
    "                title = find_paper_by_id(doc_id, kasaei_papers_info)['Title']\n",
    "                abstract = find_paper_by_id(doc_id, kasaei_papers_info)['Abstract']\n",
    "            print(\"Title: \", title[:50])\n",
    "            print(\"Abstract: \", abstract[:50])\n",
    "            print(\"*******************\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn', weight: float = 0.5,\n",
    "           print=False, preferred_field = [1, 1, 1, 1, 1]):\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        max_result_count: Return top 'max_result_count' docs which have the highest scores. \n",
    "                          notice that if max_result_count = -1, then you have to return all docs\n",
    "        \n",
    "        mode: 'detailed' for searching in title and text separately.\n",
    "              'overall' for all words, and weighted by where the word appears on.\n",
    "        \n",
    "        where: when mode ='detailed', when we want search query \n",
    "                in title or text not both of them at the same time.\n",
    "        \n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "        \n",
    "        preferred_field: A list containing preference rate to Dr. Rabiee, Dr. Soleymani, Dr. Rohban, \n",
    "                         Dr. Kasaei, and Dr. Sharifi's papers, respectively.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "        Retrieved documents with snippet\n",
    "    \"\"\"\n",
    "    # TODO: return top 'max_result_count' documents for your searched query\n",
    "    # result = [\"a72d6bd0b6d9b7aee66e91253bc6c6de37fa4e6a\"]\n",
    "\n",
    "    user_pagerank = preferred_field[0] * rabiee_pagerank + preferred_field[1] * soleymani_pagerank \n",
    "    + preferred_field[2] * rohban_pagerank + preferred_field[3] * kasaei_pagerank + preferred_field[4] * sharifi_pagerank\n",
    "    if method == 'ltn-lnn':\n",
    "        result = ltn_lnn(title_query, abstract_query, max_result_count, weight, print, user_pagerank)\n",
    "    elif method == 'ltc-lnc':\n",
    "        result = ltc_lnc(title_query, abstract_query, max_result_count, weight, print, user_pagerank)\n",
    "    elif method == 'okapi25':\n",
    "        result = okapi_bm25(title_query, abstract_query, max_result_count, weight, print, user_pagerank)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25980/184298452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"machine learning\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"machine learning\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ltn-lnn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_field\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25980/2199095366.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(title_query, abstract_query, max_result_count, method, weight, print, preferred_field)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# result = [\"a72d6bd0b6d9b7aee66e91253bc6c6de37fa4e6a\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0muser_pagerank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreferred_field\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrabiee_pagerank\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpreferred_field\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msoleymani_pagerank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;33m+\u001b[0m \u001b[0mpreferred_field\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrohban_pagerank\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpreferred_field\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkasaei_pagerank\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpreferred_field\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msharifi_pagerank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ltn-lnn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'dict'"
     ]
    }
   ],
   "source": [
    "search(\"machine learning\", \"machine learning\", 10, method='ltn-lnn', weight=0.5, print=True, preferred_field=[0.2, 0.2, 0.2, 0.2, 0.2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>  \n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = {}\n",
    "with open('crawled_paper_soleymani.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_sharifi.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_rohban.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_rabiee.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper\n",
    "with open('crawled_paper_kasaei.json') as json_file:\n",
    "    papers_info = json.load(json_file)\n",
    "    for paper in papers_info:\n",
    "        all_papers[paper['ID']] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('M. Kristan', 'Jiri Matas')\n"
     ]
    }
   ],
   "source": [
    "def hits_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        papers: A dictionary of paper dictionaries mapping the paper ID to another dictionary with the following keys:\n",
    "                \"name\": The title of the paper\n",
    "                \"abstract\": The abstract of the paper\n",
    "                \"publication_year\": The year in which the paper was published\n",
    "                \"authors\": A list of the names of the authors of the paper\n",
    "                \"related_topics\": A list of IDs for related topics (optional)\n",
    "                \"citation_count\": The number of times the paper has been cited (optional)\n",
    "                \"reference_count\": The number of references in the paper (optional)\n",
    "                \"reference_ids\": A list of IDs for papers that are cited in the paper (optional)\n",
    "        n: An integer representing the number of top authors to return.\n",
    "\n",
    "        Returns\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        List \n",
    "        list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "    STARTING_AH_VALUE = 1e-6\n",
    "    NORMALTION_FACTOR = 2\n",
    "    # Create a graph of authors and papers (all of the authors and papers represented as nodes, and all of the authors who wrote each paper connected to the corresponding paper node by an edge)\n",
    "    G, rev_G = defaultdict(list), defaultdict(list)\n",
    "    for paper in papers:\n",
    "        for author in papers[paper][\"Authors\"]:\n",
    "            G[author].append(paper)\n",
    "            rev_G[paper].append(author)\n",
    "\n",
    "    # Run the HITS algorithm\n",
    "    hubs, authorities = defaultdict(lambda: STARTING_AH_VALUE), defaultdict(lambda: STARTING_AH_VALUE)\n",
    "    for hits_iteration in range(30):\n",
    "        new_hubs, new_authorities = defaultdict(float), defaultdict(float)\n",
    "        for author in G:\n",
    "            for paper in G[author]:\n",
    "                new_hubs[author] += authorities[paper]\n",
    "            new_hubs[author] /= NORMALTION_FACTOR\n",
    "        for paper in rev_G:\n",
    "            for author in rev_G[paper]:\n",
    "                new_authorities[paper] += hubs[author]\n",
    "            new_authorities[paper] /= NORMALTION_FACTOR\n",
    "        hubs, authorities = new_hubs, new_authorities\n",
    "\n",
    "    # Create a list of top n authors based on their hub scores\n",
    "    top_authors = []\n",
    "    for author in hubs:\n",
    "        heapq.heappush(top_authors, (hubs[author], author))\n",
    "        if len(top_authors) > n:\n",
    "            heapq.heappop(top_authors)\n",
    "    return list(zip(*top_authors))[1]\n",
    "\n",
    "\n",
    "# call the hits_algorithm function\n",
    "top_authors = hits_algorithm(all_papers, 2)\n",
    "\n",
    "# print the top authors\n",
    "print(top_authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('recommended_papers.json', 'r') as fp:\n",
    "    recommended_papers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_user = recommended_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
      "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
      "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
      "['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['positive_papers'][0]['paperId'])\n",
    "print(sample_user['positive_papers'][0]['title'])\n",
    "print(sample_user['positive_papers'][0]['abstract'])\n",
    "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
      "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
      "['Computer Science', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['recommendedPapers'][0]['paperId'])\n",
    "print(sample_user['recommendedPapers'][0]['title'])\n",
    "print(sample_user['recommendedPapers'][0]['abstract'])\n",
    "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = []\n",
    "for user in recommended_papers:\n",
    "    for paper in user['positive_papers']:\n",
    "        if paper['fieldsOfStudy'] is not None:\n",
    "            fields.extend(paper['fieldsOfStudy'])\n",
    "fields = list(set(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_index (field):\n",
    "    for i in range(len(fields)):\n",
    "        if fields[i] == field:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "favorite_fields = []\n",
    "for user in recommended_papers:\n",
    "    user_favorite_fields = np.zeros(len(fields))\n",
    "    read_papers = user['positive_papers']\n",
    "    for paper in read_papers:\n",
    "        if paper['fieldsOfStudy'] is not None:\n",
    "            for field in paper['fieldsOfStudy']:\n",
    "                user_favorite_fields[get_field_index(field)] += 1\n",
    "    user_favorite_fields = user_favorite_fields / len(read_papers)\n",
    "    favorite_fields.append(user_favorite_fields)\n",
    "favorite_fields = np.array(favorite_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select randomly 70% of users for training users\n",
    "training_users = np.random.choice(len(recommended_papers), int(0.7 * len(recommended_papers)), replace=False)\n",
    "# select the rest of users for testing users\n",
    "testing_users = np.array([i for i in range(len(recommended_papers)) if i not in training_users])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure for collabrative_filtering, user_id be in the test set so we can measure the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1d36e7ba19be5db9694ed256ea21dae5f753ede3',\n",
       " '94eebbefe8a37cf394be899b85af295c2e3a1f01',\n",
       " '79f43d149cd569abf46428ed8a27a8a2b3e44a8f',\n",
       " 'ba121a6e2583c5f9b137f04324c25239c63d3473',\n",
       " '355a88ee6967af23512f85f947270cf9d81ea098',\n",
       " 'b2db53ac752045905063eb497a6a484627b037b1',\n",
       " 'dd71c525097d93d825b8abf67ce99111de421605',\n",
       " '8a5601c61fd3ab044da5eeec088b5a6a4e4b14fd',\n",
       " '45c7021c08f4c050c600886a0ceb0a7c97b1a049',\n",
       " '12ffb095891a42dd80625667a3234ded06ffa040']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collaborative_filtering(user_id: int, N=10):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on similar users (Similar users should be on \"train data\").\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "    N: The number of hyperparameter N in Nearest Neighbor algorithm.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    # select randomly\n",
    "    user_favorite_fields = favorite_fields[user_id]\n",
    "    user_similarities = []\n",
    "    for i in training_users:\n",
    "        if i == user_id:\n",
    "            continue\n",
    "        user_similarities.append((i, np.dot(user_favorite_fields, favorite_fields[i]) / \n",
    "                                  (np.linalg.norm(user_favorite_fields) * np.linalg.norm(favorite_fields[i]))))\n",
    "    user_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    similar_users = [user_similarity[0] for user_similarity in user_similarities[:N]]\n",
    "    recommended_papers_to_user = []\n",
    "    # recommend 10 top frequent papers in similar users recommended_papers\n",
    "    for user in similar_users:\n",
    "        for paper in recommended_papers[user]['recommendedPapers']:\n",
    "            recommended_papers_to_user.append(paper)\n",
    "    # count the frequency of each paper\n",
    "    paper_frequency = {}\n",
    "    for paper in recommended_papers_to_user:\n",
    "        if paper['paperId'] in paper_frequency:\n",
    "            paper_frequency[paper['paperId']] += 1\n",
    "        else:\n",
    "            paper_frequency[paper['paperId']] = 1\n",
    "    # sort the papers based on their frequency\n",
    "    sorted_paper_frequency = sorted(paper_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "    # return the top 10 papers\n",
    "    return [paper[0] for paper in sorted_paper_frequency[:10]]\n",
    "\n",
    "# call the collaborative_filtering function\n",
    "collaborative_filtering(14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recommended_papers = []\n",
    "for user in recommended_papers:\n",
    "    all_recommended_papers.extend(user['recommendedPapers'])\n",
    "# remove duplicate papers\n",
    "all_recommended_papers = list({paper['paperId']: paper for paper in all_recommended_papers}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6625a66e96821efed9b19e81d86bda7d66931020',\n",
       " '12ffb095891a42dd80625667a3234ded06ffa040',\n",
       " 'b2db53ac752045905063eb497a6a484627b037b1',\n",
       " 'c39f3752591181899f680644e8f6ce77cc5a8e3e',\n",
       " 'ee46421f518c17a51145d33a0fe7ccc6da31c51e',\n",
       " '20492f35f31e2a045a709fccc6be5cfe374c6867',\n",
       " '6e1a5efc0a45045252d358ecd3cad8ed8fa2f416',\n",
       " '7d03af1ccf5404e23bee02903b41850e88cc8590',\n",
       " '62f3ecee1135503bb2cab776e915281521ef2f3a',\n",
       " 'b981f7655c236bdd501dc5b9f234df01597a1b8c']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def content_based_recommendation(user_id):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on the titles of the articles.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    paper_titles = []\n",
    "    for paper in all_recommended_papers:\n",
    "        paper_titles.append(paper['title'])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_vectors_for_titles = vectorizer.fit_transform(paper_titles)\n",
    "    tf_idf_vectors_for_titles = tf_idf_vectors_for_titles.toarray()\n",
    "    user_positive_papers = []\n",
    "    for paper in recommended_papers[user_id]['positive_papers']:\n",
    "        user_positive_papers.append(paper['title'])\n",
    "    tf_idf_vectors_for_user_positive_papers = vectorizer.transform(user_positive_papers)\n",
    "    tf_idf_vectors_for_user_positive_papers = tf_idf_vectors_for_user_positive_papers.toarray()\n",
    "    # make a single vector for user positive papers\n",
    "    user_positive_papers_vector = np.zeros(tf_idf_vectors_for_titles.shape[1])\n",
    "    for i in range(len(user_positive_papers)):\n",
    "        user_positive_papers_vector += tf_idf_vectors_for_user_positive_papers[i]\n",
    "    user_positive_papers_vector /= len(user_positive_papers)\n",
    "    # calculate the similarity between user positive papers and all papers\n",
    "    similarity = []\n",
    "    for i in range(len(tf_idf_vectors_for_titles)):\n",
    "        similarity.append((i, np.dot(user_positive_papers_vector, tf_idf_vectors_for_titles[i]) / \n",
    "                           (np.linalg.norm(user_positive_papers_vector) * np.linalg.norm(tf_idf_vectors_for_titles[i]))))\n",
    "    similarity.sort(key=lambda x: x[1], reverse=True)\n",
    "    # return the top 10 papers\n",
    "    return [all_recommended_papers[similar_paper[0]]['paperId'] for similar_paper in similarity[:10]]\n",
    "\n",
    "\n",
    "content_based_recommendation(14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average nDCG for collaborative_filtering method on testing users: 0.4808195739207242\n",
      "average nDCG for content_based_recommendation method on testing users: 0.06527269727184194\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: Compare two methods with nDCG metric.\n",
    "\"\"\"\n",
    "\n",
    "def nDCG(user_id, method):\n",
    "    \"\"\"\n",
    "    Returns the nDCG score for the given user and method.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "    method (str): The name of the method.\n",
    "\n",
    "    Returns:\n",
    "    float: The nDCG score.\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'collaborative_filtering':\n",
    "        predicted_recommended_papers = collaborative_filtering(user_id)\n",
    "    elif method == 'content_based_recommendation':\n",
    "        predicted_recommended_papers = content_based_recommendation(user_id)\n",
    "    predicted_recommended_papers = set(predicted_recommended_papers)\n",
    "    actual_recommended_papers = set([paper['paperId'] for paper in recommended_papers[user_id]['recommendedPapers']])\n",
    "    dcg = 0\n",
    "    for i in range(len(predicted_recommended_papers)):\n",
    "        if list(predicted_recommended_papers)[i] in actual_recommended_papers:\n",
    "            if i == 0:\n",
    "                dcg += 1\n",
    "            else:\n",
    "                dcg += 1 / np.log2(i + 1)\n",
    "    idcg = 0\n",
    "    for i in range(len(actual_recommended_papers)):\n",
    "        if i == 0:\n",
    "            idcg += 1\n",
    "        else:\n",
    "            idcg += 1 / np.log2(i + 1)\n",
    "    return dcg / idcg\n",
    "# average nDCG for collaborative_filtering method on testing users\n",
    "average_nDCG_collaborative_filtering = 0\n",
    "for user in testing_users:\n",
    "    average_nDCG_collaborative_filtering += nDCG(user, 'collaborative_filtering')\n",
    "average_nDCG_collaborative_filtering /= len(testing_users)\n",
    "print(f'average nDCG for collaborative_filtering method on testing users: {average_nDCG_collaborative_filtering}')\n",
    "\n",
    "# average nDCG for content_based_recommendation method on testing users\n",
    "average_nDCG_content_based_recommendation = 0\n",
    "for user in testing_users:\n",
    "    average_nDCG_content_based_recommendation += nDCG(user, 'content_based_recommendation')\n",
    "average_nDCG_content_based_recommendation /= len(testing_users)\n",
    "print(f'average nDCG for content_based_recommendation method on testing users: {average_nDCG_content_based_recommendation}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "# Function to handle button clicks\n",
    "def button_click(button_number):\n",
    "    if button_number == 1:\n",
    "        # Open a new window for professor input\n",
    "        open_input_window()\n",
    "    elif button_number == 2:\n",
    "        # Open a new window for top authors\n",
    "        open_top_authors_window()\n",
    "    elif button_number == 3:\n",
    "        # Open a new window for collaborative filtering\n",
    "        open_collaborative_filtering_window()\n",
    "    elif button_number == 4:\n",
    "        # Open a new window for content-based filtering\n",
    "        open_content_based_filtering_window()\n",
    "\n",
    "# Function to open a new window for professor input\n",
    "def open_input_window():\n",
    "    main_window.withdraw()  # Hide the main window\n",
    "\n",
    "    input_window = tk.Toplevel()  # Create a new window\n",
    "    input_window.geometry(\"400x150\")\n",
    "    input_window.title(\"Enter Professor's Name\")\n",
    "\n",
    "    # Function to handle the input\n",
    "    def handle_input():\n",
    "        professor_name = entry.get()  # Get the input from the entry widget\n",
    "        # Call your function to get the list of important articles for the professor\n",
    "        article_ids = important_articles(professor_name)\n",
    "\n",
    "        # Create a new window to display the results\n",
    "        result_window = tk.Toplevel()  # Create a new window\n",
    "        result_window.geometry(\"500x350\")\n",
    "        result_window.title(\"Important Article IDs\")\n",
    "\n",
    "        input_window.withdraw()  # Hide the input window\n",
    "\n",
    "        # Create a label to display the article IDs\n",
    "        article_ids_label = tk.Label(result_window, text=f\"{professor_name}'s Important Article IDs:\", font=(\"Arial\", 12, \"bold\"))\n",
    "        article_ids_label.pack(pady=10)\n",
    "\n",
    "        # Create a text widget to display the article IDs\n",
    "        article_ids_text = tk.Text(result_window, height=12, width=30, font=(\"Arial\", 10))\n",
    "        article_ids_text.pack(pady=5)\n",
    "        article_ids_text.insert(tk.END, \"\\n\".join(article_ids))\n",
    "\n",
    "        # Create a back button to return to the main page\n",
    "        back_button = ttk.Button(result_window, text=\"Back\", command=lambda: go_back(result_window))\n",
    "        back_button.pack(pady=10)\n",
    "\n",
    "    label = tk.Label(input_window, text=\"Enter the professor's name:\", font=(\"Arial\", 12))\n",
    "    label.pack(pady=10)\n",
    "\n",
    "    entry = tk.Entry(input_window, width=30, font=(\"Arial\", 10))\n",
    "    entry.pack(pady=5)\n",
    "\n",
    "    submit_button = ttk.Button(input_window, text=\"Submit\", command=handle_input)\n",
    "    submit_button.pack(pady=5)\n",
    "\n",
    "    # Create a back button to return to the main page\n",
    "    back_button = ttk.Button(input_window, text=\"Back\", command=lambda: go_back(input_window))\n",
    "    back_button.pack(pady=10)\n",
    "\n",
    "# Function to open a new window for top authors\n",
    "def open_top_authors_window():\n",
    "    main_window.withdraw()  # Hide the main window\n",
    "\n",
    "    top_authors_window = tk.Toplevel()  # Create a new window\n",
    "    top_authors_window.geometry(\"400x350\")\n",
    "    top_authors_window.title(\"Top Authors\")\n",
    "\n",
    "    def handle_input():\n",
    "        # Get the input from the entry widget\n",
    "        n = int(entry.get())\n",
    "        # Call your function to get the list of top authors\n",
    "        authors = hits_algorithm(all_papers, n)\n",
    "\n",
    "        # Create a new window to display the results\n",
    "        result_window = tk.Toplevel()\n",
    "        result_window.geometry(\"500x350\")\n",
    "        result_window.title(\"Top Authors\")\n",
    "\n",
    "        top_authors_window.withdraw()  # Hide the top authors window\n",
    "\n",
    "        # Create a label to display the authors\n",
    "        authors_label = tk.Label(result_window, text=\"Top Authors:\", font=(\"Arial\", 12, \"bold\"))\n",
    "        authors_label.pack(pady=10)\n",
    "\n",
    "        # Create a text widget to display the authors\n",
    "        authors_text = tk.Text(result_window, height=12, width=30, font=(\"Arial\", 10))\n",
    "        authors_text.pack(pady=5)\n",
    "        authors_text.insert(tk.END, \"\\n\".join(authors))\n",
    "\n",
    "        # Create a back button to return to the main page\n",
    "        back_button = ttk.Button(result_window, text=\"Back\", command=lambda: go_back(result_window))\n",
    "        back_button.pack(pady=10)\n",
    "\n",
    "    label = tk.Label(top_authors_window, text=\"Enter the number of top authors:\", font=(\"Arial\", 12))\n",
    "    label.pack(pady=10)\n",
    "\n",
    "    entry = tk.Entry(top_authors_window, width=30, font=(\"Arial\", 10))\n",
    "    entry.pack(pady=5)\n",
    "\n",
    "    submit_button = ttk.Button(top_authors_window, text=\"Submit\", command=handle_input)\n",
    "    submit_button.pack(pady=5)\n",
    "\n",
    "    # Create a back button to return to the main page\n",
    "    back_button = ttk.Button(top_authors_window, text=\"Back\", command=lambda: go_back(top_authors_window))\n",
    "    back_button.pack(pady=10)\n",
    "\n",
    "# Function to open a new window for collaborative filtering\n",
    "def open_collaborative_filtering_window():\n",
    "    main_window.withdraw()  # Hide the main window\n",
    "\n",
    "    collaborative_filtering_window = tk.Toplevel()  # Create a new window\n",
    "    collaborative_filtering_window.geometry(\"400x350\")\n",
    "    collaborative_filtering_window.title(\"Collaborative Filtering\")\n",
    "\n",
    "    def handle_input():\n",
    "        # Get the input from the entry widgets\n",
    "        user_id = int(user_id_entry.get())\n",
    "        N = int(N_entry.get())\n",
    "        # Call your function to get the list of recommended articles\n",
    "        recommended_articles = collaborative_filtering(user_id, N)\n",
    "\n",
    "        # Create a new window to display the results\n",
    "        result_window = tk.Toplevel()\n",
    "        result_window.geometry(\"500x350\")\n",
    "        result_window.title(\"Recommended Articles\")\n",
    "\n",
    "        collaborative_filtering_window.withdraw()  # Hide the collaborative filtering window\n",
    "\n",
    "        # Create a label to display the recommended articles\n",
    "        recommended_articles_label = tk.Label(result_window, text=\"Recommended Articles:\", font=(\"Arial\", 12, \"bold\"))\n",
    "        recommended_articles_label.pack(pady=10)\n",
    "\n",
    "        # Create a text widget to display the recommended articles\n",
    "        recommended_articles_text = tk.Text(result_window, height=12, width=30, font=(\"Arial\", 10))\n",
    "        recommended_articles_text.pack(pady=5)\n",
    "        recommended_articles_text.insert(tk.END, \"\\n\".join(recommended_articles))\n",
    "\n",
    "        # Create a back button to return to the main page\n",
    "        back_button = ttk.Button(result_window, text=\"Back\", command=lambda: go_back(result_window))\n",
    "        back_button.pack(pady=10)\n",
    "\n",
    "    user_id_label = tk.Label(collaborative_filtering_window, text=\"Enter the user ID:\", font=(\"Arial\", 12))\n",
    "    user_id_label.pack(pady=10)\n",
    "\n",
    "    user_id_entry = tk.Entry(collaborative_filtering_window, width=30, font=(\"Arial\", 10))\n",
    "    user_id_entry.pack(pady=5)\n",
    "\n",
    "    N_label = tk.Label(collaborative_filtering_window, text=\"Enter the number of similar users:\", font=(\"Arial\", 12))\n",
    "    N_label.pack(pady=10)\n",
    "\n",
    "    N_entry = tk.Entry(collaborative_filtering_window, width=30, font=(\"Arial\", 10))\n",
    "    N_entry.pack(pady=5)\n",
    "\n",
    "    submit_button = ttk.Button(collaborative_filtering_window, text=\"Submit\", command=handle_input)\n",
    "    submit_button.pack(pady=5)\n",
    "\n",
    "    # Create a back button to return to the main page\n",
    "    back_button = ttk.Button(collaborative_filtering_window, text=\"Back\", command=lambda: go_back(collaborative_filtering_window))\n",
    "    back_button.pack(pady=10)\n",
    "\n",
    "# Function to open a new window for content-based filtering\n",
    "def open_content_based_filtering_window():\n",
    "    main_window.withdraw()  # Hide the main window\n",
    "\n",
    "    content_based_filtering_window = tk.Toplevel()  # Create a new window\n",
    "    content_based_filtering_window.geometry(\"400x350\")\n",
    "    content_based_filtering_window.title(\"Content-Based Filtering\")\n",
    "\n",
    "    def handle_input():\n",
    "        # Get the input from the entry widget\n",
    "        user_id = int(user_id_entry.get())\n",
    "        # Call your function to get the list of recommended articles\n",
    "        recommended_articles = content_based_recommendation(user_id)\n",
    "\n",
    "        # Create a new window to display the results\n",
    "        result_window = tk.Toplevel()\n",
    "        result_window.geometry(\"500x350\")\n",
    "        result_window.title(\"Recommended Articles\")\n",
    "\n",
    "        content_based_filtering_window.withdraw()  # Hide the content-based filtering window\n",
    "\n",
    "        # Create a label to display the recommended articles\n",
    "        recommended_articles_label = tk.Label(result_window, text=\"Recommended Articles:\", font=(\"Arial\", 12, \"bold\"))\n",
    "        recommended_articles_label.pack(pady=10)\n",
    "\n",
    "        # Create a text widget to display the recommended articles\n",
    "        recommended_articles_text = tk.Text(result_window, height=20, width=50, font=(\"Arial\", 10))\n",
    "        recommended_articles_text.pack(pady=5)\n",
    "        text_to_be_shown = \"\"\n",
    "        for i in range(len(recommended_articles)):\n",
    "            text_to_be_shown += f'Article {i + 1}. {recommended_articles[i]}\\n'\n",
    "        recommended_articles_text.insert(tk.END, text_to_be_shown)\n",
    "\n",
    "        # Create a back button to return to the main page\n",
    "        back_button = ttk.Button(result_window, text=\"Back\", command=lambda: go_back(result_window))\n",
    "        back_button.pack(pady=10)\n",
    "\n",
    "    user_id_label = tk.Label(content_based_filtering_window, text=\"Enter the user ID:\", font=(\"Arial\", 12))\n",
    "    user_id_label.pack(pady=10)\n",
    "\n",
    "    user_id_entry = tk.Entry(content_based_filtering_window, width=30, font=(\"Arial\", 10))\n",
    "    user_id_entry.pack(pady=5)\n",
    "\n",
    "    submit_button = ttk.Button(content_based_filtering_window, text=\"Submit\", command=handle_input)\n",
    "    submit_button.pack(pady=5)\n",
    "\n",
    "    # Create a back button to return to the main page\n",
    "    back_button = ttk.Button(content_based_filtering_window, text=\"Back\", command=lambda: go_back(content_based_filtering_window))\n",
    "    back_button.pack(pady=10)\n",
    "\n",
    "\n",
    "# Function to go back to the main page\n",
    "def go_back(window):\n",
    "    window.destroy()  # Close the current window\n",
    "    main_window.deiconify()  # Show the main window\n",
    "\n",
    "\n",
    "# Create the main GUI window\n",
    "main_window = tk.Tk()\n",
    "main_window.geometry(\"400x350\")\n",
    "main_window.title(\"UI for project features\")\n",
    "\n",
    "# Create buttons and associate them with the button_click function\n",
    "button1 = ttk.Button(main_window, text=\"Important Articles\", command=lambda: button_click(1), width=30)\n",
    "button2 = ttk.Button(main_window, text=\"Top Authors\", command=lambda: button_click(2), width=30)\n",
    "button3 = ttk.Button(main_window, text=\"Collaborative Filtering\", command=lambda: button_click(3), width=30)\n",
    "button4 = ttk.Button(main_window, text=\"Content-Based Filtering\", command=lambda: button_click(4), width=30)\n",
    "\n",
    "# Style the buttons\n",
    "button1.configure(style=\"TButton\")\n",
    "button2.configure(style=\"TButton\")\n",
    "button3.configure(style=\"TButton\")\n",
    "button4.configure(style=\"TButton\")\n",
    "\n",
    "# Position the buttons on the window\n",
    "button1.pack(pady=30)\n",
    "button2.pack(pady=10)\n",
    "button3.pack(pady=10)\n",
    "button4.pack(pady=10)\n",
    "\n",
    "# Run the GUI main loop\n",
    "main_window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
